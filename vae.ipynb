{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Sam Armstrong CS548 Project</center></h1>\n",
    "<h1><center>Variational Autoencoder with Classifier</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RNA-Seq Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_data = pd.read_csv(\"tumor_data/data.csv\")\n",
    "seq_data.rename(columns={'Unnamed: 0':'sample'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tumor Type/Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"tumor_data/labels.csv\")\n",
    "labels.rename(columns={'Unnamed: 0':'sample'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join RNA-Seq and Tumor Type/Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_data = seq_data.set_index('sample').join(labels.set_index('sample')).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pytorch Dataset for Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, transform=None):\n",
    "        'Initialization'\n",
    "        self.labels = data[:, -1]\n",
    "        self.lookup_table, self.labels = np.unique(data[:, -1], return_inverse=True)\n",
    "        self.genes = data[:, :-1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = torch.from_numpy(self.genes[index, : ].astype(np.float)).float()\n",
    "        y = torch.from_numpy(np.unique(self.labels[index]).astype(int))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.relu(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.relu(self.linear2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.hidden = []\n",
    "        self.hidden.append(torch.nn.Linear(D_in, H[0]))\n",
    "        \n",
    "        for i in range(1,len(H)):\n",
    "            self.hidden.append(torch.nn.Linear(H[i-1], H[i]))\n",
    "            \n",
    "        self.output = torch.nn.Linear(H[-1], D_out)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        for h in self.hidden:\n",
    "            x = h(x)\n",
    "       \n",
    "        return F.softmax(self.output(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder Class with Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, classifier, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self._enc_mu = torch.nn.Linear(self.encoder.linear2.out_features, latent_dim)\n",
    "        self._enc_log_sigma = torch.nn.Linear(self.encoder.linear2.out_features, latent_dim)\n",
    "\n",
    "    def _sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False)  \n",
    "    \n",
    "    \n",
    "    def forwardEncoder(self, state):\n",
    "        h_enc = self.encoder(state)\n",
    "        return self._sample_latent(h_enc)\n",
    "    \n",
    "    def forwardAutoEncoder(self, state):\n",
    "        z = self.forwardEncoder(state)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forwardClassifier(self, state):\n",
    "        z = self.forwardEncoder(state)\n",
    "        return self.classifier(z)\n",
    "    \n",
    "    def saveModel(self, fileName=\"vaeModel\"):\n",
    "        torch.save(self.state_dict(), fileName)\n",
    "        \n",
    "    def loadModel(self, fileName=\"vaeModel\"):\n",
    "        self.load_state_dict(torch.load(fileName))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Method (Kullbackâ€“Leibler Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_loss(z_mean, z_stddev):\n",
    "    mean_sq = z_mean * z_mean\n",
    "    stddev_sq = z_stddev * z_stddev\n",
    "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AutoEncoder (Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  801\n",
      "0 1.9383628368377686\n",
      "1 1.753501057624817\n",
      "2 1.763917088508606\n",
      "3 1.6090118885040283\n",
      "4 1.5195285081863403\n",
      "5 1.5446498394012451\n",
      "6 1.2940301895141602\n",
      "7 1.1701477766036987\n",
      "8 1.182070255279541\n",
      "9 1.1594533920288086\n"
     ]
    }
   ],
   "source": [
    "input_dim = 20531\n",
    "latent_dim = 8\n",
    "encoding_layer = 100\n",
    "classifier_layers = [20, 20, 20]\n",
    "classes = 5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "encoder = Encoder(input_dim, encoding_layer, encoding_layer)\n",
    "decoder = Decoder(latent_dim, encoding_layer, input_dim)\n",
    "classifier = Classifier(latent_dim, classifier_layers, classes)\n",
    "\n",
    "vae = VAE(encoder, decoder, classifier, latent_dim)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( TumorDataset(tumor_data), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('Number of samples: ', len(tumor_data))\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
    "\n",
    "l = None\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, classes = data\n",
    "        inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n",
    "        optimizer.zero_grad()\n",
    "        dec = vae.forwardAutoEncoder(inputs.float())\n",
    "        ll = latent_loss(vae.z_mean, vae.z_sigma)\n",
    "        loss = criterion(dec, inputs) + ll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l = loss.item()\n",
    "    print(epoch, l)\n",
    "# vae.saveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier (Encoder + Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  801\n",
      "0 1.6041313409805298\n",
      "1 1.6070228815078735\n"
     ]
    }
   ],
   "source": [
    "input_dim = 20531\n",
    "latent_dim = 10\n",
    "encoding_layer = 100\n",
    "classifier_layers = [20, 20, 20]\n",
    "n_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "encoder = Encoder(input_dim, encoding_layer, encoding_layer)\n",
    "decoder = Decoder(latent_dim, encoding_layer, input_dim)\n",
    "classifier = Classifier(latent_dim, classifier_layers, n_classes)\n",
    "\n",
    "vae = VAE(encoder, decoder, classifier, latent_dim)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( TumorDataset(tumor_data), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print('Number of samples: ', len(tumor_data))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n",
    "\n",
    "l = None\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, classes = data\n",
    "        inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes).flatten().long()\n",
    "        predictions = vae.forwardClassifier(inputs.float())\n",
    "        optimizer.zero_grad()\n",
    "        ll = latent_loss(vae.z_mean, vae.z_sigma)\n",
    "        loss = criterion(predictions, classes) + ll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        l = loss.item()\n",
    "    print(epoch, l)\n",
    "# vae.saveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Classifier Percentage Correct on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 19 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, classes = data\n",
    "    inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes).flatten().long()\n",
    "    predictions = vae.forwardClassifier(inputs.float())\n",
    "    _, predicted = torch.max(predictions.data, 1)\n",
    "    total += classes.size(0)\n",
    "    correct += (predicted == classes).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
